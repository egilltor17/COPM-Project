{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "d04fddb330a34ef732f1cba10e5f48b21f6eea07c886cdf995b436f2f43760b6"
        }
      }
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wslvfp2CfN",
        "outputId": "fc472fc9-7ba6-43a8-e1ba-03023c5461e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks\n",
        "![ ! -d \"/content/drive/MyDrive/Colab Notebooks/COPM-Project/\" ] && git clone https://github.com/egilltor17/COPM-Project.git\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/COPM-Project/\n",
        "!git checkout MICCAI\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "/content/drive/MyDrive/Colab Notebooks/COPM-Project\n",
            "M\tparameter.py\n",
            "M\twandb/latest-run\n",
            "Already on 'MICCAI'\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Collecting torch==1.0.1.post2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/ca/dd2c64f8ab5e7985c4af6e62da933849293906edcdb70dac679c93477733/torch-1.0.1.post2-cp36-cp36m-manylinux1_x86_64.whl (582.5MB)\n",
            "\u001b[K     |████████████████████████████████| 582.5MB 28kB/s \n",
            "\u001b[?25hCollecting pandas==0.23.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/cb/a801eaf624e36fffaa6cf1f4597a1e4b0742c200ed928e689c58fb3cb811/pandas-0.23.3-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 30.8MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
            "\u001b[K     |████████████████████████████████| 50.0MB 72kB/s \n",
            "\u001b[?25hCollecting tqdm==4.40.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/5144caf0478b1f26bd9d97f510a47336cf4ac0f96c6bc3b5af20d4173920/tqdm-4.40.2-py2.py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0e/75fbf63c3b7a14fdbfaf92ca77035c18e90963003031148211bf12441be7/scikit_image-0.13.1-cp36-cp36m-manylinux1_x86_64.whl (35.8MB)\n",
            "\u001b[K     |████████████████████████████████| 35.8MB 132kB/s \n",
            "\u001b[?25hCollecting SimpleITK==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/dc/ca09163cc405d5357c7b2198f1bbdf5ea963913e3deb6b30eb505f38a28b/SimpleITK-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (40.8MB)\n",
            "\u001b[K     |████████████████████████████████| 40.8MB 109kB/s \n",
            "\u001b[?25hCollecting pydensecrf==1.0rc3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/5a/1c2ab48e8019d282c128bc5c621332267bb954d32eecdda3ba57306b1551/pydensecrf-1.0rc3.tar.gz (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.3->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.3->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.13.1->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.13.1->-r requirements.txt (line 6)) (2.5)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.13.1->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.13.1->-r requirements.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.13.1->-r requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image==0.13.1->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image==0.13.1->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image==0.13.1->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image==0.13.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Building wheels for collected packages: pydensecrf\n",
            "  Building wheel for pydensecrf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydensecrf: filename=pydensecrf-1.0rc3-cp36-cp36m-linux_x86_64.whl size=2153644 sha256=a7551c7381d44df6c276b945fb81adaf1b12b5da976a93145029fa9624f35eb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/6f/ec/5c49c25de8c42c872de50ff53582ba3ead850ce52a81e73ac7\n",
            "Successfully built pydensecrf\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.23.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.23.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.23.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, pandas, scipy, tqdm, scikit-image, SimpleITK, pydensecrf\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-image 0.16.2\n",
            "    Uninstalling scikit-image-0.16.2:\n",
            "      Successfully uninstalled scikit-image-0.16.2\n",
            "Successfully installed SimpleITK-1.0.1 pandas-0.23.3 pydensecrf-1.0rc3 scikit-image-0.13.1 scipy-1.0.0 torch-1.0.1.post2 tqdm-4.40.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzHgOnddHqhF"
      },
      "source": [
        "# !watch nvidia-smi\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8d0xfjd1j0B"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.split(sys.path[0])[0])\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset.dataset import Dataset\n",
        "\n",
        "from loss.Dice import DiceLoss\n",
        "from loss.ELDice import ELDiceLoss\n",
        "from loss.WBCE import WCELoss\n",
        "from loss.Jaccard import JaccardLoss\n",
        "from loss.SS import SSLoss\n",
        "from loss.Tversky import TverskyLoss\n",
        "from loss.Hybrid import HybridLoss\n",
        "from loss.BCE import BCELoss\n",
        "\n",
        "from net import net\n",
        "\n",
        "import parameter as para\n",
        "\n",
        "step_list = [0]\n",
        "loss_plot = []\n",
        "\n",
        "# Set enviroment variable for GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = para.gpu\n",
        "cudnn.benchmark = para.cudnn_benchmark\n",
        "\n",
        "# Load Network\n",
        "# net = torch.nn.DataParallel(net).cuda()\n",
        "net = net.cuda()\n",
        "model_path = \"\"\n",
        "start_epoch = 0\n",
        "if len(model_path) > 0:\n",
        "    start_epoch = int(re.search(\"_net(\\d+).*\", model_path).group(1))\n",
        "    net.load_state_dict(torch.load(para.module_path + model_path))\n",
        "    net.eval()\n",
        "net.train()\n",
        "\n",
        "# Load Dateset\n",
        "train_ds = Dataset(para.training_set_path, para.training_set_path)\n",
        "\n",
        "# train_dl = DataLoader(dataset=train_ds, batch_size=para.batch_size, shuffle=True, num_workers=para.num_workers, pin_memory=para.pin_memory)\n",
        "train_dl = DataLoader(dataset=train_ds, batch_size=1, shuffle=True, pin_memory=False)\n",
        "print(\"Nr of training samples:\", len(train_dl))\n",
        "\n",
        "# Loss functions\n",
        "loss_func_list = [DiceLoss(), ELDiceLoss(), WCELoss(), JaccardLoss(), SSLoss(), TverskyLoss(), HybridLoss(), BCELoss()]\n",
        "loss_func = loss_func_list[5]\n",
        "\n",
        "# Define Optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=para.learning_rate)\n",
        "\n",
        "# Learning rate decay\n",
        "lr_decay = torch.optim.lr_scheduler.MultiStepLR(opt, para.learning_rate_decay)\n",
        "\n",
        "# In-depth supervision attenuation coefficient\n",
        "alpha = para.alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5qPk33jlWmj"
      },
      "source": [
        "# Training the network\n",
        "print(finally\"Training epochs: {start_epoch}-{para.Epoch}\")\n",
        "start = time()\n",
        "for epoch in range(start_epoch, para.Epoch+1):\n",
        "    mean_loss = []\n",
        "    for step, (ct, seg) in enumerate(train_dl):\n",
        "        # Half input resolution\n",
        "        s = np.array(range(0, ct.shape(-1), 2))\n",
        "        ct = ct[:,:,:,s,:]\n",
        "        ct = ct[:,:,:,:,s]\n",
        "\n",
        "        ct = ct.cuda()\n",
        "        seg = seg.cuda()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        outputs = net(ct)\n",
        "    \n",
        "        loss1 = loss_func(outputs[0], seg)\n",
        "        loss2 = loss_func(outputs[1], seg)\n",
        "        loss3 = loss_func(outputs[2], seg)\n",
        "        loss4 = loss_func(outputs[3], seg)\n",
        "        loss = (loss1 + loss2 + loss3) * alpha + loss4\n",
        "        mean_loss.append(loss4.item())\n",
        "\n",
        "        # opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if step % 5 is 0:\n",
        "            step_list.append(step_list[-1] + 1)\n",
        "            loss_plot.append(loss4.item())\n",
        "\n",
        "            print('epoch:{}, step:{}, loss1:{:.3f}, loss2:{:.3f}, loss3:{:.3f}, loss4:{:.3f}, time:{:.3f} min'\n",
        "                  .format(epoch, step, loss1.item(), loss2.item(), loss3.item(), loss4.item(), (time() - start) / 60))\n",
        "            \n",
        "    # Save model\n",
        "    if epoch % 25 is 0 and epoch is not 0:\n",
        "        torch.save(net.state_dict(), para.module_path + '_net{}-{:.3f}-{:.3f}.pth'.format(epoch, loss, sum(mean_loss) / len(mean_loss)))\n",
        "\n",
        "    # Attenuate the depth supervision coefficient\n",
        "    if epoch % 40 is 0 and epoch is not 0:\n",
        "        alpha *= 0.8\n",
        "\n",
        "    lr_decay.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXi4nsQyqs8I"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onuvB517F8Sj"
      },
      "source": [
        "!pwd\n",
        "!git status\n",
        "!echo -e \"[user]\\n\\tname = egilltor17\\n\\temail = egilltor17@ru.is\" > ~/.gitconfig\n",
        "!git add .\n",
        "!git commit -m \"Changes from Google Colab\"\n",
        "#!git pull origin MICCAI\n",
        "!git push origin MICCAI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB1tS9HeFKDZ",
        "outputId": "23df19c2-09a5-4654-e29b-980678164ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "### Validation ###\n",
        "import os\n",
        "import copy\n",
        "import collections\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.ndimage as ndimage\n",
        "import SimpleITK as sitk\n",
        "import skimage\n",
        "import skimage.measure as measure\n",
        "import skimage.morphology as morphology\n",
        "\n",
        "\n",
        "from net.ResUNet import UNet\n",
        "from utilities.calculate_metrics import Metirc\n",
        "\n",
        "import parameter as para\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = para.gpu\n",
        "\n",
        "# In order to calculate the two variables defined by dice_global\n",
        "dice_intersection = 0.0  \n",
        "dice_union = 0.0\n",
        "\n",
        "file_name = []\n",
        "time_pre_case = []\n",
        "\n",
        "# Loss functions\n",
        "liver_score = collections.OrderedDict()\n",
        "liver_score['dice'] = []\n",
        "liver_score['jacard'] = []\n",
        "liver_score['voe'] = []\n",
        "liver_score['fnr'] = []\n",
        "liver_score['fpr'] = []\n",
        "liver_score['assd'] = []\n",
        "liver_score['rmsd'] = []\n",
        "liver_score['msd'] = []\n",
        "\n",
        "# Define network and load parameters\n",
        "# net = torch.nn.DataParallel(UNet(training=False)).cuda()\n",
        "model_path = \"net1000\"\n",
        "net = UNet(training=False).cuda()\n",
        "net.load_state_dict(torch.load(para.module_path + model_path))\n",
        "net.eval()\n",
        "\n",
        "for file_index, file in enumerate(os.listdir(para.test_ct_path)):\n",
        "\n",
        "    start = time()\n",
        "\n",
        "    file_name.append(file)\n",
        "\n",
        "    # Read CT-volume\n",
        "    ct = sitk.ReadImage(os.path.join(para.test_ct_path, file), sitk.sitkInt16)\n",
        "    ct_array = sitk.GetArrayFromImage(ct)\n",
        "\n",
        "    origin_shape = ct_array.shape\n",
        "    \n",
        "    # Truncate the gray value outside the threshold\n",
        "    ct_array[ct_array > para.upper] = para.upper\n",
        "    ct_array[ct_array < para.lower] = para.lower\n",
        "\n",
        "    # min max Normalization\n",
        "    ct_array = ct_array.astype(np.float32)\n",
        "    ct_array = ct_array / 200\n",
        "\n",
        "    # Interpolate CT using bicubic algorithm, the array after interpolation is still int16\n",
        "    ct_array = ndimage.zoom(ct_array, (1, para.down_scale, para.down_scale), order=3)\n",
        "\n",
        "    # Use padding for data with too few slices\n",
        "    too_small = False\n",
        "    if ct_array.shape[0] < para.size:\n",
        "        depth = ct_array.shape[0]\n",
        "        temp = np.ones((para.size, int(512 * para.down_scale), int(512 * para.down_scale))) * para.lower\n",
        "        temp[0: depth] = ct_array\n",
        "        ct_array = temp \n",
        "        too_small = True\n",
        "\n",
        "    # Sliding window sampling prediction\n",
        "    start_slice = 0\n",
        "    end_slice = start_slice + para.size - 1\n",
        "    count = np.zeros((ct_array.shape[0], 512, 512), dtype=np.int16)\n",
        "    probability_map = np.zeros((ct_array.shape[0], 512, 512), dtype=np.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while end_slice < ct_array.shape[0]:\n",
        "\n",
        "            ct_tensor = torch.FloatTensor(ct_array[start_slice: end_slice + 1]).cuda()\n",
        "            ct_tensor = ct_tensor.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            outputs = net(ct_tensor)\n",
        "\n",
        "            count[start_slice: end_slice + 1] += 1\n",
        "            probability_map[start_slice: end_slice + 1] += np.squeeze(outputs.cpu().detach().numpy())\n",
        "\n",
        "            # Due to insufficient video memory, the ndarray data is directly retained here, and the calculation graph is directly destroyed after saving\n",
        "            del outputs      \n",
        "            \n",
        "            start_slice += para.stride\n",
        "            end_slice = start_slice + para.size - 1\n",
        "    \n",
        "        if end_slice != ct_array.shape[0] - 1:\n",
        "            end_slice = ct_array.shape[0] - 1\n",
        "            start_slice = end_slice - para.size + 1\n",
        "\n",
        "            ct_tensor = torch.FloatTensor(ct_array[start_slice: end_slice + 1]).cuda()\n",
        "            ct_tensor = ct_tensor.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "            outputs = net(ct_tensor)\n",
        "\n",
        "            count[start_slice: end_slice + 1] += 1\n",
        "            probability_map[start_slice: end_slice + 1] += np.squeeze(outputs.cpu().detach().numpy())\n",
        "\n",
        "            del outputs\n",
        "        \n",
        "        pred_seg = np.zeros_like(probability_map)\n",
        "        pred_seg[probability_map >= (para.threshold * count)] = 1\n",
        "\n",
        "        if too_small:\n",
        "            temp = np.zeros((depth, 512, 512), dtype=np.float32)\n",
        "            temp += pred_seg[0: depth]\n",
        "            pred_seg = temp\n",
        "\n",
        "    # Read the ground truth into memory\n",
        "    seg = sitk.ReadImage(os.path.join(para.test_seg_path, file.replace('volume', 'segmentation')), sitk.sitkUInt8)\n",
        "    seg_array = sitk.GetArrayFromImage(seg)\n",
        "    seg_array[seg_array > 0] = 1\n",
        "\n",
        "    # Extract the largest connected domain of the liver, remove small areas, and fill in internal holes\n",
        "    pred_seg = pred_seg.astype(np.uint8)\n",
        "    liver_seg = copy.deepcopy(pred_seg)\n",
        "    liver_seg = measure.label(liver_seg, 4)\n",
        "    props = measure.regionprops(liver_seg)\n",
        "    \n",
        "    max_area = 0\n",
        "    max_index = 0\n",
        "    for index, prop in enumerate(props, start=1):\n",
        "        if prop.area > max_area:\n",
        "            max_area = prop.area\n",
        "            max_index = index\n",
        "    \n",
        "    liver_seg[liver_seg != max_index] = 0\n",
        "    liver_seg[liver_seg == max_index] = 1\n",
        "    \n",
        "    liver_seg = liver_seg.astype(np.bool)\n",
        "    morphology.remove_small_holes(liver_seg, para.maximum_hole, connectivity=2, in_place=True)\n",
        "    liver_seg = liver_seg.astype(np.uint8)\n",
        "\n",
        "    # Calculate segmentation evaluation index\n",
        "    liver_metric = Metirc(seg_array, liver_seg, ct.GetSpacing())\n",
        "\n",
        "    liver_score['dice'].append(liver_metric.get_dice_coefficient()[0])\n",
        "    liver_score['jacard'].append(liver_metric.get_jaccard_index())\n",
        "    liver_score['voe'].append(liver_metric.get_VOE())\n",
        "    liver_score['fnr'].append(liver_metric.get_FNR())\n",
        "    liver_score['fpr'].append(liver_metric.get_FPR())\n",
        "    liver_score['assd'].append(liver_metric.get_ASSD())\n",
        "    liver_score['rmsd'].append(liver_metric.get_RMSD())\n",
        "    liver_score['msd'].append(liver_metric.get_MSD())\n",
        "\n",
        "    dice_intersection += liver_metric.get_dice_coefficient()[1]\n",
        "    dice_union += liver_metric.get_dice_coefficient()[2]\n",
        "\n",
        "    # Save the prediction as .nii\n",
        "    pred_seg = sitk.GetImageFromArray(liver_seg)\n",
        "\n",
        "    pred_seg.SetDirection(ct.GetDirection())\n",
        "    pred_seg.SetOrigin(ct.GetOrigin())\n",
        "    pred_seg.SetSpacing(ct.GetSpacing())\n",
        "\n",
        "    sitk.WriteImage(pred_seg, os.path.join(para.pred_path, file.replace('volume', 'pred')))\n",
        "\n",
        "    speed = time() - start\n",
        "    time_pre_case.append(speed)\n",
        "\n",
        "    print(file_index, 'this case use {:.3f} s'.format(speed))\n",
        "    print('-----------------------')\n",
        "\n",
        "\n",
        "# Write evaluation indicators into exel\n",
        "liver_data = pd.DataFrame(liver_score, index=file_name)\n",
        "liver_data['time'] = time_pre_case\n",
        "\n",
        "liver_statistics = pd.DataFrame(index=['mean', 'std', 'min', 'max'], columns=list(liver_data.columns))\n",
        "liver_statistics.loc['mean'] = liver_data.mean()\n",
        "liver_statistics.loc['std'] = liver_data.std()\n",
        "liver_statistics.loc['min'] = liver_data.min()\n",
        "liver_statistics.loc['max'] = liver_data.max()\n",
        "\n",
        "writer = pd.ExcelWriter('./result.xlsx')\n",
        "liver_data.to_excel(writer, 'liver')\n",
        "liver_statistics.to_excel(writer, 'liver_statistics')\n",
        "writer.save()\n",
        "\n",
        "# Print dice global\n",
        "print('dice global:', dice_intersection / dice_union)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1f5524959967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleITK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0m_raise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapply_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marraycrop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_regular_grid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregular_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregular_seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/util/arraycrop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraypad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_validate_lengths'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}