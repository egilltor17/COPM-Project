{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "d04fddb330a34ef732f1cba10e5f48b21f6eea07c886cdf995b436f2f43760b6"
        }
      }
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wslvfp2CfN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks\n",
        "![ ! -d \"/content/drive/MyDrive/Colab Notebooks/COPM-Project/\" ] && git clone https://github.com/egilltor17/COPM-Project.git\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/COPM-Project/\n",
        "!git checkout MICCAI\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzHgOnddHqhF"
      },
      "source": [
        "# !watch nvidia-smi\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8d0xfjd1j0B"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.split(sys.path[0])[0])\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset.dataset import Dataset\n",
        "\n",
        "from loss.Dice import DiceLoss\n",
        "from loss.ELDice import ELDiceLoss\n",
        "from loss.WBCE import WCELoss\n",
        "from loss.Jaccard import JaccardLoss\n",
        "from loss.SS import SSLoss\n",
        "from loss.Tversky import TverskyLoss\n",
        "from loss.Hybrid import HybridLoss\n",
        "from loss.BCE import BCELoss\n",
        "\n",
        "from net import net\n",
        "\n",
        "import parameter as para\n",
        "\n",
        "step_list = [0]\n",
        "loss_plot = []\n",
        "\n",
        "# Set enviroment variable for GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = para.gpu\n",
        "cudnn.benchmark = para.cudnn_benchmark\n",
        "\n",
        "# Load Network\n",
        "# net = torch.nn.DataParallel(net).cuda()\n",
        "net = net.cuda()\n",
        "model_path = \"\"\n",
        "start_epoch = 0\n",
        "if len(model_path) > 0:\n",
        "    start_epoch = int(re.search(\"_net(\\d+).*\", model_path).group(1))\n",
        "    net.load_state_dict(torch.load(para.module_path + model_path))\n",
        "    net.eval()\n",
        "net.train()\n",
        "\n",
        "# Load Dateset\n",
        "train_ds = Dataset(para.training_set_path, para.training_set_path)\n",
        "\n",
        "# train_dl = DataLoader(dataset=train_ds, batch_size=para.batch_size, shuffle=True, num_workers=para.num_workers, pin_memory=para.pin_memory)\n",
        "train_dl = DataLoader(dataset=train_ds, batch_size=1, shuffle=True, pin_memory=False)\n",
        "print(\"Nr of training samples:\", len(train_dl))\n",
        "\n",
        "# Loss functions\n",
        "loss_func_list = [DiceLoss(), ELDiceLoss(), WCELoss(), JaccardLoss(), SSLoss(), TverskyLoss(), HybridLoss(), BCELoss()]\n",
        "loss_func = loss_func_list[5]\n",
        "\n",
        "# Define Optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=para.learning_rate)\n",
        "\n",
        "# Learning rate decay\n",
        "lr_decay = torch.optim.lr_scheduler.MultiStepLR(opt, para.learning_rate_decay)\n",
        "\n",
        "# In-depth supervision attenuation coefficient\n",
        "alpha = para.alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5qPk33jlWmj"
      },
      "source": [
        "# Training the network\n",
        "print(finally\"Training epochs: {start_epoch}-{para.Epoch}\")\n",
        "start = time()\n",
        "for epoch in range(start_epoch, para.Epoch+1):\n",
        "    mean_loss = []\n",
        "    for step, (ct, seg) in enumerate(train_dl):\n",
        "        # Half input resolution\n",
        "        s = np.array(range(0, ct.shape(-1), 2))\n",
        "        ct = ct[:,:,:,s,:]\n",
        "        ct = ct[:,:,:,:,s]\n",
        "\n",
        "        ct = ct.cuda()\n",
        "        seg = seg.cuda()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        outputs = net(ct)\n",
        "    \n",
        "        loss1 = loss_func(outputs[0], seg)\n",
        "        loss2 = loss_func(outputs[1], seg)\n",
        "        loss3 = loss_func(outputs[2], seg)\n",
        "        loss4 = loss_func(outputs[3], seg)\n",
        "        loss = (loss1 + loss2 + loss3) * alpha + loss4\n",
        "        mean_loss.append(loss4.item())\n",
        "\n",
        "        # opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if step % 5 is 0:\n",
        "            step_list.append(step_list[-1] + 1)\n",
        "            loss_plot.append(loss4.item())\n",
        "\n",
        "            print('epoch:{}, step:{}, loss1:{:.3f}, loss2:{:.3f}, loss3:{:.3f}, loss4:{:.3f}, time:{:.3f} min'\n",
        "                  .format(epoch, step, loss1.item(), loss2.item(), loss3.item(), loss4.item(), (time() - start) / 60))\n",
        "            \n",
        "    # Save model\n",
        "    if epoch % 25 is 0 and epoch is not 0:\n",
        "        torch.save(net.state_dict(), para.module_path + '_net{}-{:.3f}-{:.3f}.pth'.format(epoch, loss, sum(mean_loss) / len(mean_loss)))\n",
        "\n",
        "    # Attenuate the depth supervision coefficient\n",
        "    if epoch % 40 is 0 and epoch is not 0:\n",
        "        alpha *= 0.8\n",
        "\n",
        "    lr_decay.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXi4nsQyqs8I"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onuvB517F8Sj"
      },
      "source": [
        "!pwd\n",
        "!git status\n",
        "!echo -e \"[user]\\n\\tname = egilltor17\\n\\temail = egilltor17@ru.is\" > ~/.gitconfig\n",
        "!git add .\n",
        "!git commit -m \"Changes from Google Colab\"\n",
        "#!git pull origin MICCAI\n",
        "!git push origin MICCAI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzl0qFMcGpW"
      },
      "source": [
        "!pip install --upgrade scikit-image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB1tS9HeFKDZ"
      },
      "source": [
        "### Validation ###\n",
        "import os\n",
        "import copy\n",
        "import collections\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.ndimage as ndimage\n",
        "import SimpleITK as sitk\n",
        "import skimage\n",
        "import skimage.measure as measure\n",
        "import skimage.morphology as morphology\n",
        "\n",
        "\n",
        "from net import ResUNet as UNet\n",
        "from utilities.calculate_metrics import Metirc\n",
        "\n",
        "import parameter as para\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = para.gpu\n",
        "\n",
        "# In order to calculate the two variables defined by dice_global\n",
        "dice_intersection = 0.0  \n",
        "dice_union = 0.0\n",
        "\n",
        "file_name = []\n",
        "time_pre_case = []\n",
        "\n",
        "# Loss functions\n",
        "liver_score = collections.OrderedDict()\n",
        "liver_score['dice'] = []\n",
        "liver_score['jacard'] = []\n",
        "liver_score['voe'] = []\n",
        "liver_score['fnr'] = []\n",
        "liver_score['fpr'] = []\n",
        "liver_score['assd'] = []\n",
        "liver_score['rmsd'] = []\n",
        "liver_score['msd'] = []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvfLx6suar6w"
      },
      "source": [
        "# Define network and load parameters\n",
        "# net = torch.nn.DataParallel(UNet(training=False)).cuda()\n",
        "model_path = \"net1000-1.792-0.558.pth\"\n",
        "net = UNet(training=False).cuda()\n",
        "net.load_state_dict(torch.load(para.module_path + model_path))\n",
        "net.eval()\n",
        "\n",
        "for file_index, file in enumerate(os.listdir(para.test_ct_path)):\n",
        "    start = time()\n",
        "    print(file_index, start)\n",
        "    file_name.append(file)\n",
        "\n",
        "    # Read CT-volume\n",
        "    ct = sitk.ReadImage(os.path.join(para.test_ct_path, file), sitk.sitkInt16)\n",
        "    ct_array = sitk.GetArrayFromImage(ct)\n",
        "\n",
        "    origin_shape = ct_array.shape\n",
        "    \n",
        "    # Truncate the gray value outside the threshold\n",
        "    ct_array[ct_array > para.upper] = para.upper\n",
        "    ct_array[ct_array < para.lower] = para.lower\n",
        "\n",
        "    # min max Normalization\n",
        "    ct_array = ct_array.astype(np.float32)\n",
        "    ct_array = ct_array / 200\n",
        "\n",
        "    # Interpolate CT using bicubic algorithm, the array after interpolation is still int16\n",
        "    ct_array = ndimage.zoom(ct_array, (1, para.down_scale, para.down_scale), order=3)\n",
        "\n",
        "    # Use padding for data with too few slices\n",
        "    too_small = False\n",
        "    if ct_array.shape[0] < para.size:\n",
        "        depth = ct_array.shape[0]\n",
        "        temp = np.ones((para.size, int(512 * para.down_scale), int(512 * para.down_scale))) * para.lower\n",
        "        temp[0: depth] = ct_array\n",
        "        ct_array = temp \n",
        "        too_small = True\n",
        "\n",
        "    # Sliding window sampling prediction\n",
        "    start_slice = 0\n",
        "    end_slice = start_slice + para.size - 1\n",
        "    count = np.zeros((ct_array.shape[0], 512, 512), dtype=np.int16)\n",
        "    probability_map = np.zeros((ct_array.shape[0], 512, 512), dtype=np.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while end_slice < ct_array.shape[0]:\n",
        "\n",
        "            ct_tensor = torch.FloatTensor(ct_array[start_slice: end_slice + 1]).cuda()\n",
        "            ct_tensor = ct_tensor.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "            outputs = net(ct_tensor)\n",
        "\n",
        "            count[start_slice: end_slice + 1] += 1\n",
        "            probability_map[start_slice: end_slice + 1] += np.squeeze(outputs.cpu().detach().numpy())\n",
        "\n",
        "            # Due to insufficient video memory, the ndarray data is directly retained here, and the calculation graph is directly destroyed after saving\n",
        "            del outputs      \n",
        "            \n",
        "            start_slice += para.stride\n",
        "            end_slice = start_slice + para.size - 1\n",
        "    \n",
        "        if end_slice != ct_array.shape[0] - 1:\n",
        "            end_slice = ct_array.shape[0] - 1\n",
        "            start_slice = end_slice - para.size + 1\n",
        "\n",
        "            ct_tensor = torch.FloatTensor(ct_array[start_slice: end_slice + 1]).cuda()\n",
        "            ct_tensor = ct_tensor.unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "            outputs = net(ct_tensor)\n",
        "\n",
        "            count[start_slice: end_slice + 1] += 1\n",
        "            probability_map[start_slice: end_slice + 1] += np.squeeze(outputs.cpu().detach().numpy())\n",
        "\n",
        "            del outputs\n",
        "        \n",
        "        pred_seg = np.zeros_like(probability_map)\n",
        "        pred_seg[probability_map >= (para.threshold * count)] = 1\n",
        "\n",
        "        if too_small:\n",
        "            temp = np.zeros((depth, 512, 512), dtype=np.float32)\n",
        "            temp += pred_seg[0: depth]\n",
        "            pred_seg = temp\n",
        "\n",
        "    # Read the ground truth into memory\n",
        "    seg = sitk.ReadImage(os.path.join(para.test_seg_path, file.replace('volume', 'segmentation')), sitk.sitkUInt8)\n",
        "    seg_array = sitk.GetArrayFromImage(seg)\n",
        "    seg_array[seg_array > 0] = 1\n",
        "\n",
        "    # Extract the largest connected domain of the liver, remove small areas, and fill in internal holes\n",
        "    pred_seg = pred_seg.astype(np.uint8)\n",
        "    liver_seg = copy.deepcopy(pred_seg)\n",
        "    liver_seg = measure.label(liver_seg, 4)\n",
        "    props = measure.regionprops(liver_seg)\n",
        "    \n",
        "    max_area = 0\n",
        "    max_index = 0\n",
        "    for index, prop in enumerate(props, start=1):\n",
        "        if prop.area > max_area:\n",
        "            max_area = prop.area\n",
        "            max_index = index\n",
        "    \n",
        "    liver_seg[liver_seg != max_index] = 0\n",
        "    liver_seg[liver_seg == max_index] = 1\n",
        "    \n",
        "    liver_seg = liver_seg.astype(np.bool)\n",
        "    morphology.remove_small_holes(liver_seg, para.maximum_hole, connectivity=2, in_place=True)\n",
        "    liver_seg = liver_seg.astype(np.uint8)\n",
        "\n",
        "    # Calculate segmentation evaluation index\n",
        "    liver_metric = Metirc(seg_array, liver_seg, ct.GetSpacing())\n",
        "\n",
        "    liver_score['dice'].append(liver_metric.get_dice_coefficient()[0])\n",
        "    liver_score['jacard'].append(liver_metric.get_jaccard_index())\n",
        "    liver_score['voe'].append(liver_metric.get_VOE())\n",
        "    liver_score['fnr'].append(liver_metric.get_FNR())\n",
        "    liver_score['fpr'].append(liver_metric.get_FPR())\n",
        "    liver_score['assd'].append(liver_metric.get_ASSD())\n",
        "    liver_score['rmsd'].append(liver_metric.get_RMSD())\n",
        "    liver_score['msd'].append(liver_metric.get_MSD())\n",
        "\n",
        "    dice_intersection += liver_metric.get_dice_coefficient()[1]\n",
        "    dice_union += liver_metric.get_dice_coefficient()[2]\n",
        "\n",
        "    # Save the prediction as .nii\n",
        "    pred_seg = sitk.GetImageFromArray(liver_seg)\n",
        "\n",
        "    pred_seg.SetDirection(ct.GetDirection())\n",
        "    pred_seg.SetOrigin(ct.GetOrigin())\n",
        "    pred_seg.SetSpacing(ct.GetSpacing())\n",
        "\n",
        "    sitk.WriteImage(pred_seg, os.path.join(para.pred_path, file.replace('volume', 'pred')))\n",
        "\n",
        "    speed = time() - start\n",
        "    time_pre_case.append(speed)\n",
        "\n",
        "    print(file_index, 'this case use {:.3f} s'.format(speed))\n",
        "    print('-----------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQUJ8iewarEH"
      },
      "source": [
        "# Write evaluation indicators into exel\n",
        "liver_data = pd.DataFrame(liver_score, index=file_name)\n",
        "liver_data['time'] = time_pre_case\n",
        "\n",
        "liver_statistics = pd.DataFrame(index=['mean', 'std', 'min', 'max'], columns=list(liver_data.columns))\n",
        "liver_statistics.loc['mean'] = liver_data.mean()\n",
        "liver_statistics.loc['std'] = liver_data.std()\n",
        "liver_statistics.loc['min'] = liver_data.min()\n",
        "liver_statistics.loc['max'] = liver_data.max()\n",
        "\n",
        "writer = pd.ExcelWriter('./result.xlsx')\n",
        "liver_data.to_excel(writer, 'liver')\n",
        "liver_statistics.to_excel(writer, 'liver_statistics')\n",
        "writer.save()\n",
        "\n",
        "# Print dice global\n",
        "print('dice global:', dice_intersection / dice_union)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}